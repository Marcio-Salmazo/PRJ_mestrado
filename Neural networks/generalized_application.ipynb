{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''        \n",
    "    Importação das bibliotecas necessárias para a contrução e configuração das redes neurais convolucionais:\n",
    "\n",
    "    1 - TensorFlow: amplamente usada para criar e treinar modelos de aprendizado profundo\n",
    "    2 - models: fornece classes e funções para construir, configurar e treinar modelos de redes neurais\n",
    "    3 - layers: contém uma variedade de classes que representam diferentes tipos de camadas em redes neurais. \n",
    "    4 - TensorBoard:  ferramenta de visualização do TensorFlow que ajuda a monitorar e visualizar o comportamento dos modelos\n",
    "    5 - train_test_split:  usada para dividir conjuntos de dados em subconjuntos de treino e teste.\n",
    "    \n",
    "    Importação das bibliotecas necessárias para a manipulção dos dados e arrays:\n",
    "    \n",
    "    1 - numpy: fornece suporte para arrays e matrizes de grandes dimensões\n",
    "    2 - pandas: biblioteca para manipulação e análise de dados em Python\n",
    "    3 - pyplot: usada para criar gráficos e visualizações em Python\n",
    "    4 - cv2: biblioteca de visão computacional e aprendizado de máquina.\n",
    "    5 - os: biblioteca padrão do Python que fornece uma maneira de interagir \n",
    "        com o sistema operacional. Ela inclui funções para manipulação de arquivos, \n",
    "        diretórios, e processos.\n",
    "    6 - glob: biblioteca padrão do Python que facilita a busca por arquivos em diretórios com base em padrões especificados, \n",
    "        usando caracteres curinga como * e ?.\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''       \n",
    "    Definição da classe responsável por construir as redes neurais utilizadas.\n",
    "    Dentro da classe, existem as seguintes funções:\n",
    "    \n",
    "    * __init__: construtor padrão da classe, neste caso ele não opera nenhuma \n",
    "      função  em específico\n",
    "    * AlexNet: constrói um modelo de rede neural densa, baseada na arquitetura AlexNet\n",
    "    * ShallowNet: constrói um modelo de rede neural raso, contendo pocas camadas \n",
    "      com poucas unidades\n",
    "'''\n",
    "class NeuralNetworks:\n",
    "\n",
    "    def __init__(self, loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def AlexNet(self, input_shape = (512, 512, 3), dataTrain = None, classTrain = None, batch_size = 32, epochs = 5, valData = None):\n",
    "        \n",
    "        self.model = Sequential([\n",
    "\n",
    "            Conv2D(96, (11, 11), strides=4, activation='relu', input_shape = input_shape, padding='valid'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "\n",
    "            Conv2D(256, (5, 5), strides=1, activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "                \n",
    "            Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'),    \n",
    "            Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'),\n",
    "            Conv2D(256, (3, 3), strides=1, activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "                \n",
    "            Flatten(),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(units = 1, activation = 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        board = TensorBoard(log_dir='./logsAlex')\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\n",
    "        self.model.fit(dataTrain, classTrain, batch_size = batch_size, epochs = epochs, validation_data = valData, callbacks=[board])\n",
    "    \n",
    "\n",
    "    def ShallowNet(self, input_shape = (512, 512, 3), dataTrain = None, classTrain = None, batch_size = 32, epochs = 5, valData = None):\n",
    "        \n",
    "        self.model = Sequential([\n",
    "\n",
    "            Conv2D(32, (3,3), input_shape = input_shape, activation = 'relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size = (2,2)),\n",
    "            Conv2D(32, (3,3), activation = 'relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size = (2,2)),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(units = 128, activation = 'relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(units = 128, activation = 'relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(units = 1, activation = 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        board = TensorBoard(log_dir='./logsShallow')\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\n",
    "        self.model.fit(dataTrain, classTrain, batch_size = batch_size, epochs = epochs, validation_data = valData, callbacks=[board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''       \n",
    "    Definição da classe responsável por manipular os datasets\n",
    "    sendo composta pelas seguintes funções:\n",
    "    \n",
    "    * getImages(): responsável por carregar a pasta contendo as imagens\n",
    "      dos ovos. Para cada um dos arquivos encontrados é feita a leitura\n",
    "      do arquivo .jpg (imagem), as imagens lidas são armazenadas\n",
    "      na lista 'data'\n",
    "\n",
    "    * dataSplit(): responsável por realizar a divisão dos dados entre\n",
    "      subsets para treinamento e teste da rede. Além disso ela também\n",
    "      realiza a normalização dos valores (caso seja especificado no parâmetro)\n",
    "'''\n",
    "\n",
    "class DataProcessing:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getImages(self, ImagePath, csvPath):\n",
    "\n",
    "        # Carregamento das imagens:\n",
    "        # img_dir = \"C:/Users/marci/Desktop/Projeto mestrado/CNN Egg application/Egg Dataset\"\n",
    "        # img_dir = \"C:/Users/marci/Desktop/Arquivos/PRJ_mestrado/CNN Egg application/Egg Dataset\"\n",
    "        data_path = os.path.join(ImagePath,'*g') \n",
    "\n",
    "        folder = glob.glob(data_path) \n",
    "        data = [] \n",
    "        for files in folder: \n",
    "            img = cv2.imread(files) \n",
    "            data.append(img) \n",
    "\n",
    "        # Carregamento do .csv\n",
    "        eggClass = pd.read_csv(csvPath)\n",
    "        cList = eggClass.to_numpy()\n",
    "\n",
    "        return data, cList\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "\n",
    "    def dataSplit(self, data, cList, perc, norm = 0):\n",
    "\n",
    "        # Divisão do dataset entre subsets para treino e teste\n",
    "        # A divisão é feita levando em consideração os mesmos índices\n",
    "        ind = np.arange(len(data))\n",
    "        train, test = train_test_split(ind, test_size=perc, random_state=42)\n",
    "\n",
    "        dataTrain = []\n",
    "        dataTest = []\n",
    "        classTrain = []\n",
    "        classTest = []\n",
    "\n",
    "        for i in range(len(train)):\n",
    "\n",
    "            dataTrain.append(data[train[i]])\n",
    "            classTrain.append(cList[train[i]])\n",
    "\n",
    "        for j in range(len(test)):\n",
    "\n",
    "            dataTest.append(data[test[j]])\n",
    "            classTest.append(cList[test[j]])\n",
    "\n",
    "        dataTrain = np.array(dataTrain)\n",
    "        dataTest = np.array(dataTest)\n",
    "        classTrain = np.array(classTrain)\n",
    "        classTest = np.array(classTest)\n",
    "\n",
    "        # Processo de normalização\n",
    "        if norm == 1:\n",
    "\n",
    "            dataTrain = dataTrain.astype('float32')\n",
    "            dataTest = dataTest.astype('float32')\n",
    "\n",
    "            dataTrain /= 255\n",
    "            dataTest /= 255\n",
    "\n",
    "            return dataTrain, dataTest, classTrain, classTest\n",
    "        \n",
    "        else:\n",
    "\n",
    "            return dataTrain, dataTest, classTrain, classTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet para inicialização dos dados necessários\n",
    "models = NeuralNetworks()\n",
    "imgPath = 'C:/Users/marci/Desktop/Remastered Project/Images Dataset/Batch 06.04 Images'\n",
    "csvPath = 'C:/Users/marci/Desktop/Remastered Project/Datasets/Batch 06.04/RealData.csv'\n",
    "\n",
    "dataPro = DataProcessing()\n",
    "imgData, csvData = dataPro.getImages(imgPath, csvPath)\n",
    "dataTrain, dataTest, classTrain, classTest = dataPro.dataSplit(imgData, csvData, perc = 0.2, norm = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csvPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcsvPath\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m imgPath\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dataPro\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csvPath' is not defined"
     ]
    }
   ],
   "source": [
    "del csvPath\n",
    "del imgPath\n",
    "del dataPro\n",
    "del imgData\n",
    "del csvData\n",
    "del dataPro\n",
    "\n",
    "print(len(dataTrain))\n",
    "print(len(dataTest))\n",
    "print(len(classTrain))\n",
    "print(len(classTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet responsável pelo treinamento das redes\n",
    "alexNet = models.AlexNet(dataTrain = dataTrain,\n",
    "                         classTrain= classTrain,\n",
    "                         batch_size = 32,\n",
    "                         epochs = 5,\n",
    "                         valData = (dataTest, classTest))\n",
    "\n",
    "# Salvar os pesos de um modelo, comentar caso não seja necessário\n",
    "alexNet.save_weights('alex.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marci\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.6271 - loss: 24.4163"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.99 GiB for an array with shape (1020, 512, 512, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shallowNet \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShallowNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataTrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mclassTrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclassTrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalData\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassTest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Salvar os pesos de um modelo, comentar caso não seja necessário\u001b[39;00m\n\u001b[0;32m      8\u001b[0m shallowNet\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshallow.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mNeuralNetworks.ShallowNet\u001b[1;34m(self, input_shape, dataTrain, classTrain, batch_size, epochs, valData)\u001b[0m\n\u001b[0;32m     67\u001b[0m board \u001b[38;5;241m=\u001b[39m TensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logsShallow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.99 GiB for an array with shape (1020, 512, 512, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "shallowNet = models.ShallowNet(dataTrain = dataTrain,\n",
    "                            classTrain= classTrain,\n",
    "                            batch_size = 32,\n",
    "                            epochs = 1,\n",
    "                            valData = (dataTest, classTest))\n",
    "\n",
    "# Salvar os pesos de um modelo, comentar caso não seja necessário\n",
    "shallowNet.save_weights('shallow.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
