{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''        \n",
    "    Importação das bibliotecas necessárias para a contrução e configuração das redes neurais convolucionais:\n",
    "\n",
    "    1 - TensorFlow: amplamente usada para criar e treinar modelos de aprendizado profundo\n",
    "    2 - models: fornece classes e funções para construir, configurar e treinar modelos de redes neurais\n",
    "    3 - layers: contém uma variedade de classes que representam diferentes tipos de camadas em redes neurais. \n",
    "    4 - TensorBoard:  ferramenta de visualização do TensorFlow que ajuda a monitorar e visualizar o comportamento dos modelos\n",
    "    5 - train_test_split:  usada para dividir conjuntos de dados em subconjuntos de treino e teste.\n",
    "    \n",
    "    Importação das bibliotecas necessárias para a manipulção dos dados e arrays:\n",
    "    \n",
    "    1 - numpy: fornece suporte para arrays e matrizes de grandes dimensões\n",
    "    2 - pandas: biblioteca para manipulação e análise de dados em Python\n",
    "    3 - pyplot: usada para criar gráficos e visualizações em Python\n",
    "    4 - cv2: biblioteca de visão computacional e aprendizado de máquina.\n",
    "    5 - os: biblioteca padrão do Python que fornece uma maneira de interagir \n",
    "        com o sistema operacional. Ela inclui funções para manipulação de arquivos, \n",
    "        diretórios, e processos.\n",
    "    6 - glob: biblioteca padrão do Python que facilita a busca por arquivos em diretórios com base em padrões especificados, \n",
    "        usando caracteres curinga como * e ?.\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''       \n",
    "    Definição da classe responsável por construir as redes neurais utilizadas.\n",
    "    Dentro da classe, existem as seguintes funções:\n",
    "    \n",
    "    * __init__: construtor padrão da classe, neste caso ele não opera nenhuma \n",
    "      função  em específico\n",
    "    * AlexNet: constrói um modelo de rede neural densa, baseada na arquitetura AlexNet\n",
    "    * ShallowNet: constrói um modelo de rede neural raso, contendo pocas camadas \n",
    "      com poucas unidades\n",
    "'''\n",
    "class NeuralNetworks:\n",
    "\n",
    "    def __init__(self, loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def AlexNet(self, input_shape = (512, 512, 3), dataTrain = None, classTrain = None, batch_size = 32, epochs = 5, valData = None):\n",
    "        \n",
    "        self.model = Sequential([\n",
    "\n",
    "            Conv2D(96, (11, 11), strides=4, activation='relu', input_shape = input_shape, padding='valid'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "\n",
    "            Conv2D(256, (5, 5), strides=1, activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "                \n",
    "            Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'),    \n",
    "            Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'),\n",
    "            Conv2D(256, (3, 3), strides=1, activation='relu', padding='same'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "                \n",
    "            Flatten(),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(4096, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(units = 1, activation = 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        board = TensorBoard(log_dir='./logsAlex')\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\n",
    "        self.model.fit(dataTrain, classTrain, batch_size = batch_size, epochs = epochs, validation_data = valData, callbacks=[board])\n",
    "    \n",
    "\n",
    "    def ShallowNet(self, input_shape = (512, 512, 3), dataTrain = None, classTrain = None, batch_size = 32, epochs = 5, valData = None):\n",
    "        \n",
    "        self.model = Sequential([\n",
    "\n",
    "            Conv2D(32, (3,3), input_shape = input_shape, activation = 'relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size = (2,2)),\n",
    "            Conv2D(32, (3,3), activation = 'relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size = (2,2)),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(units = 128, activation = 'relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(units = 128, activation = 'relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(units = 1, activation = 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        board = TensorBoard(log_dir='./logsShallow')\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\n",
    "        self.model.fit(dataTrain, classTrain, batch_size = batch_size, epochs = epochs, validation_data = valData, callbacks=[board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''       \n",
    "    Definição da classe responsável por manipular os datasets\n",
    "    sendo composta pelas seguintes funções:\n",
    "    \n",
    "    * getImages(): responsável por carregar a pasta contendo as imagens\n",
    "      dos ovos. Para cada um dos arquivos encontrados é feita a leitura\n",
    "      do arquivo .jpg (imagem), as imagens lidas são armazenadas\n",
    "      na lista 'data'\n",
    "\n",
    "    * dataSplit(): responsável por realizar a divisão dos dados entre\n",
    "      subsets para treinamento e teste da rede. Além disso ela também\n",
    "      realiza a normalização dos valores (caso seja especificado no parâmetro)\n",
    "'''\n",
    "\n",
    "class DataProcessing:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getImages(self, ImagePath, csvPath):\n",
    "\n",
    "        # Carregamento das imagens:\n",
    "        # img_dir = \"C:/Users/marci/Desktop/Projeto mestrado/CNN Egg application/Egg Dataset\"\n",
    "        # img_dir = \"C:/Users/marci/Desktop/Arquivos/PRJ_mestrado/CNN Egg application/Egg Dataset\"\n",
    "        data_path = os.path.join(ImagePath,'*g') \n",
    "\n",
    "        folder = glob.glob(data_path) \n",
    "        data = [] \n",
    "        for files in folder: \n",
    "            img = cv2.imread(files) \n",
    "            data.append(img) \n",
    "\n",
    "        # Carregamento do .csv\n",
    "        eggClass = pd.read_csv(csvPath)\n",
    "        cList = eggClass.to_numpy()\n",
    "\n",
    "        return data, cList\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "\n",
    "    def dataSplit(self, data, cList, perc, norm = 0):\n",
    "\n",
    "        # Divisão do dataset entre subsets para treino e teste\n",
    "        # A divisão é feita levando em consideração os mesmos índices\n",
    "        ind = np.arange(len(data))\n",
    "        train, test = train_test_split(ind, test_size=perc, random_state=42)\n",
    "\n",
    "        dataTrain = []\n",
    "        dataTest = []\n",
    "        classTrain = []\n",
    "        classTest = []\n",
    "\n",
    "        for i in range(len(train)):\n",
    "\n",
    "            dataTrain.append(data[train[i]])\n",
    "            classTrain.append(cList[train[i]])\n",
    "\n",
    "        for j in range(len(test)):\n",
    "\n",
    "            dataTest.append(data[test[j]])\n",
    "            classTest.append(cList[test[j]])\n",
    "\n",
    "        dataTrain = np.array(dataTrain)\n",
    "        dataTest = np.array(dataTest)\n",
    "        classTrain = np.array(classTrain)\n",
    "        classTest = np.array(classTest)\n",
    "\n",
    "        # Processo de normalização\n",
    "        if norm == 1:\n",
    "\n",
    "            dataTrain = dataTrain.astype('float32')\n",
    "            dataTest = dataTest.astype('float32')\n",
    "\n",
    "            dataTrain /= 255\n",
    "            dataTest /= 255\n",
    "\n",
    "            return dataTrain, dataTest, classTrain, classTest\n",
    "        \n",
    "        else:\n",
    "\n",
    "            return dataTrain, dataTest, classTrain, classTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet para inicialização dos dados necessários\n",
    "models = NeuralNetworks()\n",
    "imgPath = 'C:/Users/marci/Desktop/Remastered Project/Images Dataset/Batch 06.04 Images'\n",
    "csvPath = 'C:/Users/marci/Desktop/Remastered Project/Datasets/Batch 06.04/RealData.csv'\n",
    "\n",
    "dataPro = DataProcessing()\n",
    "imgData, csvData = dataPro.getImages(imgPath, csvPath)\n",
    "dataTrain, dataTest, classTrain, classTest = dataPro.dataSplit(imgData, csvData, perc = 0.2, norm = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del csvPath\n",
    "del imgPath\n",
    "del dataPro\n",
    "del imgData\n",
    "del csvData\n",
    "del dataPro\n",
    "\n",
    "print(len(dataTrain))\n",
    "print(len(dataTest))\n",
    "print(len(classTrain))\n",
    "print(len(classTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet responsável pelo treinamento das redes\n",
    "alexNet = models.AlexNet(dataTrain = dataTrain,\n",
    "                         classTrain= classTrain,\n",
    "                         batch_size = 32,\n",
    "                         epochs = 5,\n",
    "                         valData = (dataTest, classTest))\n",
    "\n",
    "# Salvar os pesos de um modelo, comentar caso não seja necessário\n",
    "alexNet.save_weights('alex.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallowNet = models.ShallowNet(dataTrain = dataTrain,\n",
    "                            classTrain= classTrain,\n",
    "                            batch_size = 32,\n",
    "                            epochs = 1,\n",
    "                            valData = (dataTest, classTest))\n",
    "\n",
    "# Salvar os pesos de um modelo, comentar caso não seja necessário\n",
    "shallowNet.save_weights('shallow.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
