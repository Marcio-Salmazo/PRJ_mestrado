{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n",
      "(5100, 1)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    Carregando a pasta de imagens (Dataset dos ovos)\n",
    "    * A função glob encontra todos os nomes de caminhos que correspondem a um padrão \n",
    "      especificado de acordo com as regras usadas pelo shell Unix\n",
    "\n",
    "    * Para cada um dos arquivos encontrados é feita a leitura\n",
    "      do arquivo .jpg (imagem), as imagens lidas são armazenadas\n",
    "      na lista 'data'\n",
    "'''\n",
    "\n",
    "img_dir = \"C:/Users/marci/Desktop/Projeto mestrado/CNN Egg application/Egg Dataset\"\n",
    "data_path = os.path.join(img_dir,'*g') \n",
    "\n",
    "folder = glob.glob(data_path) \n",
    "data = [] \n",
    "for files in folder: \n",
    "    img = cv2.imread(files) \n",
    "    data.append(img) \n",
    "\n",
    "'''   \n",
    "    Carregando e tratando o arquivo CSV\n",
    "'''\n",
    "eggClass = pd.read_csv(\"RealData.csv\")\n",
    "cList = eggClass.to_numpy()\n",
    "\n",
    "print(len(data))\n",
    "print(cList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2091 2705 4947 ... 3556 1581  801]\n",
      "class 2000\n",
      "images 2000\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = np.random.choice(np.arange(5100), 2000, replace = False)\n",
    "print(sample)\n",
    "\n",
    "imgList = []\n",
    "clsList = []\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    imgList.append(data[sample[i]])\n",
    "    clsList.append(cList[sample[i]])\n",
    "\n",
    "print('class', len(clsList))\n",
    "print('images', len(imgList))\n",
    "print('------------------------------------------')\n",
    "#print('class', clsList)\n",
    "#print('images', imgList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img train 1600\n",
      "img test 400\n",
      "class train 1600\n",
      "class test 400\n"
     ]
    }
   ],
   "source": [
    "ind = np.arange(len(clsList))\n",
    "train, test = train_test_split(ind, test_size=0.2, random_state=42)\n",
    "\n",
    "dataTrain = []\n",
    "dataTest = []\n",
    "classTrain = []\n",
    "classTest = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "\n",
    "    dataTrain.append(data[train[i]])\n",
    "    classTrain.append(cList[train[i]])\n",
    "\n",
    "for j in range(len(test)):\n",
    "\n",
    "    dataTest.append(data[test[j]])\n",
    "    classTest.append(cList[test[j]])\n",
    "\n",
    "dataTrain = np.array(dataTrain)\n",
    "dataTest = np.array(dataTest)\n",
    "classTrain = np.array(classTrain)\n",
    "classTest = np.array(classTest)\n",
    "\n",
    "print('img train', len(dataTrain))\n",
    "print('img test', len(dataTest))\n",
    "print('class train', len(classTrain))\n",
    "print('class test', len(classTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Alterando o tipo dos dados para float32 a fim de aplicar \n",
    "    a normalização futuramente\n",
    "'''\n",
    "\n",
    "dataTrain = dataTrain.astype('float32')\n",
    "dataTest = dataTest.astype('float32')\n",
    "\n",
    "'''\n",
    "    Realizando a normalização (min/max normalization) a fim de que os valores dos pixels estejam\n",
    "    entre 0 e 1, tornando o processamento mais eficiente\n",
    "    obs: 255 é o valor máximo do pixel\n",
    "'''\n",
    "\n",
    "dataTrain /= 255\n",
    "dataTest /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Camada convolucional 1\n",
    "model.add(Conv2D(96, (11, 11), strides=4, activation='relu', input_shape = (512, 512, 3), padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "    \n",
    "# Camada convolucional 2\n",
    "model.add(Conv2D(256, (5, 5), strides=1, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "    \n",
    "# Camada convolucional 3\n",
    "model.add(Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'))\n",
    "    \n",
    "# Camada convolucional\n",
    "model.add(Conv2D(384, (3, 3), strides=1, activation='relu', padding='same'))\n",
    "    \n",
    "# Camada convolucional 5\n",
    "model.add(Conv2D(256, (3, 3), strides=1, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "    \n",
    "# Camada Densa da rede\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "   # Definição da rede neural convolucional com arquitetura AlexNet\n",
    "\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Camada Convolucional 1\n",
    "classifier.add(Conv2D(96, (11,11), input_shape = (512, 512, 3), activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "classifier.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "    # Adição das hidden Layers\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    # Compilação da rede neural\n",
    "\n",
    "classifier.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\marci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "100/100 [==============================] - 573s 6s/step - loss: 12.7008 - accuracy: 0.6862 - val_loss: 0.6541 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 561s 6s/step - loss: 0.5967 - accuracy: 0.7594 - val_loss: 0.5786 - val_accuracy: 0.7500\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 574s 6s/step - loss: 0.5629 - accuracy: 0.7594 - val_loss: 0.5648 - val_accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 580s 6s/step - loss: 0.5548 - accuracy: 0.7594 - val_loss: 0.5626 - val_accuracy: 0.7500\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 586s 6s/step - loss: 0.5532 - accuracy: 0.7594 - val_loss: 0.5623 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1aca3d4cbd0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "    Etapa de treinamento da rede neural. É importante que a função 'fit_generator' está sendo\n",
    "    utilizada ao invés da função 'fit' uma vez que ela suporta o processo de augmentation, contudo\n",
    "    tal função está em processo de depreciação, uma vez que em versões mais atuais, a função 'fit' \n",
    "    também suporta.\n",
    "\n",
    "    Explicação dos parâmetros:\n",
    "\n",
    "    * trainDatabase -> Dados para treino (após a augmentation)\n",
    "    * steps_per_epoch -> Número total de etapas (lotes de amostras) a serem produzidas \n",
    "                         pelo gerador antes de declarar uma época concluída e iniciar a próxima época.\n",
    "                         É importante citar que o valor ideal para este parâmetro se dá pela quantidade \n",
    "                         total de imagens para treinamento (caso haja um alto poder de processamento) ou\n",
    "                         pelo total de amostras dividido pel valor do batch_size (caso haja um baixo \n",
    "                         poder de processamento)\n",
    "    * epochs -> Epocas de treinamento da rede\n",
    "    * validation_data -> Dados para a validação (após a augmentation)\n",
    "    * validation_steps -> Possui o mesmo princípio do 'steps_per_epoch', porém levando em \n",
    "                          consideração a etapa de validação. O valor ideal para este parâmetro \n",
    "                          se dá pelo total de amostras dividido pel valor do batch_size\n",
    "'''\n",
    "board = TensorBoard(log_dir='./logs')\n",
    "model.fit(dataTrain, classTrain, batch_size = 16, epochs = 30, validation_data = (dataTest, classTest), callbacks=[board])\n",
    "#classifier.fit(dataTrain, classTrain, batch_size = 16, epochs = 5, validation_data = (dataTest, classTest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
